{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 200px; display: inline\" alt=\"Python\"/></a> [pour Statistique et Science des Données](https://github.com/wikistat/Intro-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Statistique  multidimensionnelle avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> & <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 180px; display: inline\" alt=\"Scikit-Learn\"/></a>\n",
    "**Résumé**: Après la préparation des données, ce calepin introduit l'utilisation de la librairie `scikit-learn` pour l'exploration statistique. Liste des fonctionnalités, quelques exemples de mise en oeuvre: ([ACP](http://wikistat.fr/pdf/st-m-explo-acp.pdf), [AFCM](http://wikistat.fr/pdf/st-m-explo-afcm.pdf), [$k$-means](http://wikistat.fr/pdf/st-m-explo-classif.pdf)). Des fonctionalités plus avancées de `Scikit-learn` sont abordés dans d'autres calepins du [dépôt d'exploration](https://github.com/wikistat/Exploration) statistique des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 `Scikit-learn` *vs.*  R\n",
    "L'objectif de ce tutoriel est d'introduire l'utilisation de la librairie `scikit-learn` de Python pour l'exploration de données multidimensionnelles. Seule l'utilisation directe des fonctions d'exploration est abordée. Se pose rapidement une question: quand utiliser `scikit-learn` de Python plutôt que R plus complet et plus simple d'emploi?\n",
    "\n",
    "Le choix repose sur les points suivants:\n",
    "- **Attention** cette librairie manipule des objets de classe `array` de `numpy` *chargés en mémoire* et donc de taille limitée par la RAM de l'ordinateur; de façon analogue R charge en RAM des objets de type `data.frame`.\n",
    "- **Attention** toujours, `scikit-learn` (0.18) ne reconnaît pas (ou pas encore ?) la classe `DataFrame` de `pandas`; `scikit-learn` utilise la classe `array` de `numpy`. C'est un problème pour la gestion de variables qualitatives complexes. Une variable binaire est simplement remplacée par un codage $(0,1)$ mais, en présence de plusieurs modalités, traiter celles-ci comme des entiers n'a pas de sens statistique et remplacer une variable qualitative par l'ensemble des indicatrices (*dummy variables* $(0,1)$) de ses modalités  complique l'interprétation statistique. \n",
    "- Les implémentations en Python de certains algorithmes dans `scikit-learn` sont aussi efficaces (*e.g.* $k$-means), voire beaucoup plus efficaces pour des données volumineuses car utilisent implicitement les capacités de parallélisation.\n",
    "- R offre beaucoup plus de possibilités pour une exploration, des recherches et comparaisons, des interprétations mais les capacités de parallélisation de Python sont nettement plus performantes. Plus précisément, l'introduction de nouvelles librairies n'est pas ou peu contraintes dans R, alors que celle de nouvelles méthodes dans `scikit-learn` se fait sous contrôle d'un groupe qui en contrôle la pertinence et l'efficacité. \n",
    "\n",
    "En conséquences:\n",
    "- Préférer R et ses libraires si la présentation graphique des résultats et surtout leur interprétation est prioritaire.\n",
    "- POur l'emploi de méthodes (analyse factorielle discriminante, canonique, positionnement multidimensionnel...) pas codées en Python.\n",
    "- Préférer Python et `scikit-learn` pour mettre au point une chaîne de traitements (*pipe line*) opérationnelle de l'extraction à une analyse privilégiant la prévision brute à l'interprétation et pour des données quantitatives ou rendues quantitatives (\"vectorisation\" de corpus de textes).\n",
    "\n",
    "### 1.2 Fonctions de `scikit-learn`\n",
    "La communauté qui développe cette librairie est très active, elle évolue rapidement.  Ne pas hésiter à consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) pour des compléments. Voici une sélection de ses principales fonctionnalités.\n",
    "- Transformations (standardisation, discrétisation binaire, regroupement de modalités, imputations rudimentaires de données manquantes) , \"vectorisation\" de corpus de textes (encodage, catalogue, Tf-idf), images.\n",
    "- Exploration: ACP, classification non supervisée (mélanges gaussiens, propagation d'affinité, ascendante hiérarchique, SOM,...). Une fonction est aojutée pour l'Analyse des Correspondances.\n",
    "- Modélisation et apprentissage, voir le calepin correspondant.\n",
    "\n",
    "\n",
    "### 1.3 Objectif\n",
    "L'objectif est d'illustrer la mise en oeuvre de quelques fonctionnalités. Consulter la [documentation](http://scikit-learn.org/stable/user_guide.html) et ses nombreux [exemples](http://scikit-learn.org/stable/auto_examples/index.html) pour plus de détails sur l'utilisation de `scikit-learn`. \n",
    "\n",
    "Deux jeux de données élémentaires sont utilisés. Celui déjà étudié avec `pandas` et concernant le naufrage du [Titanic](http://wikistat.fr/Notebooks/Cal2-PythonPanda.html) mélange des variables explicatives qualitatives et quantitatives dans un objet de la classe `DataFrame`. Pour être utilisé dans `scikit-learn` les données doivent être transformées en un objet de classe `Array` de `numpy` en remplaçant les variables qualitatives par les indicatrices de leurs modalités.  L'autre ensemble de données est entièrement quantitatif. C'est un problème classique et simplifié de [reconnaissance de caractères](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) qui est inclus dans la librairie `scikit-learn`.\n",
    "\n",
    "Ces données sont explorées par [analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) (caractères) ou [analyse multiple des correspondances](http://wikistat.fr/pdf/st-m-explo-afcm.pdf) (titanic), [classifiées](http://wikistat.fr/pdf/st-m-explo-classif.pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Les données\n",
    "### 2.1 Les données \"Caractères\"\n",
    "Il s'agit d'explorer celles de reconnaissance de caractères dont les procédés d'obtention et prétraitements sont décrits sur le [site de l'UCI](http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits) (Lichman, 2013). Les chiffres ont été saisies sur des tablettes à l'intérieur de cadres de résolution $500\\times 500$. Des procédures de normalisation,  ré-échantillonnage spatial puis de lissage ont été appliquées. Chaque caractère apparaît finalement discrétisé sous la forme d'une matrice $8\\times 8$ de pixels à 16 niveaux de gris et identifié par un label. Les données sont archivées sous la forme d'une matrice ou tableau à trois indices. Elles sont également archivées après vectorisation des images sous la forme d'une matrice à $p=64$ colonnes.\n",
    "\n",
    "L'étude du même type de données mais nettement plus complexes (MNIST): 60 000 caractères représentés par des images de 784 pixels (26 $\\times$ 26) fait l'objet d'un autre calepin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importations \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "# les données\n",
    "digits = datasets.load_digits()\n",
    "# Contenu et mode d'obtention\n",
    "print(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sous forme d'un cube d'images 1797 x 8x8\n",
    "print(digits.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sous forme d'une matrice 1797 x 64\n",
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label réel de chaque caractère\n",
    "print(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un aperçu des images à décrire puis discriminer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images, \n",
    "   digits.target))\n",
    "for index, (image, label) in  enumerate(images_and_labels[:8]):\n",
    "     plt.subplot(2, 4, index + 1)\n",
    "     plt.axis('off')\n",
    "     plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "     plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Données Titanic\n",
    "Les données sur le naufrage du Titanic sont décrites dans le calepin consacré à `pandas`. Reconstruire la table de données en lisant le fichier `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lire les données d'apprentissage\n",
    "import pandas as pd\n",
    "path=''  # si les données sont déjà dans le répertoire courant\n",
    "#path='http://www.math.univ-toulouse.fr/~besse/Wikistat/data/'\n",
    "df=pd.read_csv(path+'titanic-train.csv',skiprows=1,header=None,usecols=[1,2,4,5,9,11],\n",
    "  names=[\"Surv\",\"Classe\",\"Genre\",\"Age\",\"Prix\",\"Port\"],dtype={\"Surv\":object,\"Classe\":object,\"Genre\":object,\"Port\":object})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape # dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redéfinir les types \n",
    "df[\"Surv\"]=pd.Categorical(df[\"Surv\"],ordered=False)\n",
    "df[\"Classe\"]=pd.Categorical(df[\"Classe\"],ordered=False)\n",
    "df[\"Genre\"]=pd.Categorical(df[\"Genre\"],ordered=False)\n",
    "df[\"Port\"]=pd.Categorical(df[\"Port\"],ordered=False)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier que les données contiennent des valeurs manquantes, faire des imputations à la médiane d'une valeur quantitative manquante ou la modalité la plus fréquente d'une valeur qualitative absente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imputation des valeurs manquantes\n",
    "df[\"Age\"]=df[\"Age\"].fillna(df[\"Age\"].median())\n",
    "df.Port=df[\"Port\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuer en transformant les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrétiser les variables quantitatives\n",
    "df[\"AgeQ\"]=pd.qcut(df.Age,3,labels=[\"Ag1\",\"Ag2\",\"Ag3\"])\n",
    "df[\"PrixQ\"]=pd.qcut(df.Prix,3,labels=[\"Pr1\",\"Pr2\",\"Pr3\"])\n",
    "# redéfinir les noms des modalités \n",
    "df[\"Surv\"]=df[\"Surv\"].cat.rename_categories([\"Vnon\",\"Voui\"])\n",
    "df[\"Classe\"]=df[\"Classe\"].cat.rename_categories([\"Cl1\",\"Cl2\",\"Cl3\"])\n",
    "df[\"Genre\"]=df[\"Genre\"].cat.rename_categories([\"Gfem\",\"Gmas\"])\n",
    "df[\"Port\"]=df[\"Port\"].cat.rename_categories([\"Pc\",\"Pq\",\"Ps\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Analyse en composantes principales\n",
    "La fonction d'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) (ACP) est surtout adaptée à l'analyse de signaux, de nombreuses options ne sont pas disponibles notamment les graphiques usuels (biplot, cercle des corrélations...). En revanche des résultats sont liés à la version probabiliste de l'ACP sous hypothèse d'une distribution gaussienne multidimensionnelle des données. **Attention**, l'ACP est évidemment centrée mais par réduite. L'option n'est pas prévue et les variables doivent être réduites (fonction `sklearn.preprocessing.scale`) avant si c'est nécessaire. L'attribut `transform` désigne les composantes principales, sous-entendu: transformation par réduction de la dimension; `n_components` fixe le nombre de composantes retenues, par défaut toutes; l'attribut `components_` contient les `n_components` vecteurs propres mais non normalisés, c'est-à-dire de norme carrée la valeur propre associée, et donc à utiliser pour représenter les variables.\n",
    "\n",
    "D'autres versions d'analyse en composantes principales sont proposées dans *Scikit-learn*: *kernel PCA, sparse PCA*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X=digits.data\n",
    "y=digits.target\n",
    "target_name=[0,1,2,3,4,5,6,7,8,9]\n",
    "# définition de la commande\n",
    "pca = PCA()\n",
    "# Estimation, calcul des composantes principales\n",
    "C = pca.fit(X).transform(X)\n",
    "# Décroissance de la variance expliquée\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagramme boîte des premières composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(C[:,0:20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation des caractères dans le premier plan principal. \n",
    "\n",
    "La représentation des variables (pixels) et le *biplot* n'ont pas grand intérêt pour ces données. Les commandes de construction du *biplot* sont présentées dans un [autre calepin](https://github.com/wikistat/Exploration/tree/master/GRC-carte_Visa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(C[:,0], C[:,1], c=y, label=target_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le même graphique avec une légende mais moins de couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attention aux indentations\n",
    "plt.figure()\n",
    "for c, i, target_name in zip(\"rgbcmykrgb\",[0,1,2,3,4,5,6,7,8,9], target_name):\n",
    "       plt.scatter(C[y == i,0], C[y == i,1], c=c, label=target_name)\n",
    "plt.legend()\n",
    "plt.title(\"ACP Digits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphique en trois dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], c=y, cmap=plt.cm.Paired)\n",
    "ax.set_title(\"ACP: trois premieres composantes\")\n",
    "ax.set_xlabel(\"Comp1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Comp2\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Comp3\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification non supervisée\n",
    "De nombreuses méthodes de classification non supervisée sont décrite dans la [documentation](http://scikit-learn.org/stable/modules/clustering.html#clustering) de *Scikit-learn*: *k*-means, *affinity propagation*, *mean-shift*, *sectral clustering*, *agglomerative clustering* ou classificaiton ascendante hiérarchique (CAH avec saut complet, moyen ou de Ward) , DBSCA, mélanges gaussiens, *Birch*. Seules *k*-means et la CAH sont illustrées ci-dessous. D'autres exemples sont ou seront développés dans les calepins du [dépot d'exploration](https://github.com/wikistat/Exploration).\n",
    "\n",
    "### 4.1 Classification par ré-allocation dynamique\n",
    "Exécution de l'algorithme de [classification non supervisée](http://wikistat.fr/pdf/st-m-explo-classif.pdf) (*clustering*) $k$-means dans un cas simple: le nombre des classes : paramètre `n_clusters` (8 par défaut) est *a priori* connu. D'autres paramètres peuvent être précisés ou laissés à leur valeur par défaut. Le nombre `max_iter` d'itérations (300), le nombre `n_init` (10) d'exécutions parmi lesquelles la meilleure en terme de minimisation de la variance intra-classe est retenue, le mode `init` d'initialisation qui peut être `k-means++` (par défaut) pour en principe accélérer la convergence, `random` parmi les données, ou une matrice déterminant les centres initiaux. \n",
    "\n",
    "L'option `n_jobs` permet d'exécuter les `n_init` en parallèle. Pour la valeur -1, tous les processeurs sauf un sont utilisés.\n",
    "\n",
    "Les attributs en sortie contiennent les centres: `cluster_centers_`, les numéros de classe de chaque observation: `labels_`. \n",
    "\n",
    "De nombreux critères à consulter dans la documentation sont proposés pour évaluer la qualité d'une classification non-supervisée.\n",
    "\n",
    "On se contente des options par défaut dans cette illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "est=KMeans(n_clusters=10)\n",
    "est.fit(X)\n",
    "classe=est.labels_\n",
    "print(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table=pd.crosstab(classe,y)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vraies classes étant connues, il est facile de construire une matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(table)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'est pas plus difficile de représenter les classes dans les coordonnées de l'ACP. C'est la variable `classe` qui définit les couleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], c=classe,cmap=plt.cm.Paired)\n",
    "ax.set_title(\"ACP: trois premieres composantes\")\n",
    "ax.set_xlabel(\"Comp1\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"Comp2\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"Comp3\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Classification ascendante hiérarchique\n",
    "La fonction [`AgglomerativeClustering`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) de *Scikit-learn* produit bien une classificaiton ascendante hiérarchique mais ne semble pas offrir la possibilité de tracer le dendrogramme, ce qui est assez ennyeux. Le dendrogramme doit être recherché dans la librairie [*scipy*](http://www.scipy.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linkage_matrix = linkage(X, 'ward')\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    truncate_mode=\"lastp\",  # Ne montre que les p dernières classes\n",
    "    p=40,  # Ne représente que les 40 dernières classes\n",
    "    show_leaf_counts=True,  # effectifs  des feuilles\n",
    "    leaf_rotation=60.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True,  \n",
    ")\n",
    "plt.title('Dendrogramme')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Analyse multiple des correspondances\n",
    "Les données \"Titanic\" regroupent des variables qualitatives et quantitatives. Après recodage en classes (discrétisation) des variables quantitatives, la table obtenue se prête à une [analyse](http://wikistat.fr/pdf/st-m-explo-afcm.pdf) factorielle multiple des correspondances}. Cette méthode n'est pas présente dans les librairies courantes de python mais disponible sous la forme d'une fonction [`mca.py`](https://pypi.python.org/pypi/mca/1.0). Il est possible d'installer la librairie correspondante après l'avoir chargée, par la commande `pip install --user mca` selon l'installation, ou de simplement charger le seul module [`mca.py`](http://wikistat/programmes/mca.py) dans le répertoire courant. \n",
    "\n",
    "*Remarque*, il ne serait pas très compliqué de recalculer directement les composantes de l'AFCM à partir de la SVD du tableau disjonctif complet obtenu en remplaçant chaque variables par les indicatrices (*dummy variables*) de ses modalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppression des variables quantitatives\n",
    "# pour l'AFCM\n",
    "df_q=df.drop([\"Age\",\"Prix\"],axis=1)\n",
    "df_q.head()\n",
    "# Indicatrices\n",
    "dc=pd.DataFrame(pd.get_dummies(df_q[[\"Surv\",\"Classe\",\"Genre\",\"Port\",\"AgeQ\",\"PrixQ\"]]))\n",
    "dc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de l'AFCM et représentations graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mca import mca\n",
    "mca_df=mca(dc,benzecri=False)\n",
    "# Valeurs singulières\n",
    "print(mca_df.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Composantes principales des colonnes (modalités)\n",
    "print(mca_df.fs_c())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Premier plan principal\n",
    "col=[1,1,2,2,2,3,3,5,5,5,6,6,6,7,7,7]\n",
    "plt.scatter(mca_df.fs_c()[:, 0],mca_df.fs_c()[:, 1],c=col)\n",
    "for i, j, nom in zip(mca_df.fs_c()[:, 0],mca_df.fs_c()[:, 1], dc.columns):\n",
    "       plt.text(i, j, nom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour l'ACP et au contraire de R (cf. [`FactoMineR`](http://factominer.free.fr/)), les librairies Python sont pauvres en fonctions graphiques directement adaptées à l'AFCM. Le graphique est construit à partir des fonctions de `MatPlotLib`. Remarquer l'évidente redondance entre la variable `Prix` et celle `Classe`. Il serait opportun d'en déclarer une supplémentaire. \n",
    "\n",
    "Il est alors facile de construire des classifications non supervisées des modalités des variables ou des passagers à partir de leurs composantes respectives quantitatives, ce qui revient à considérer des distances dites du $\\chi^2$ entre ces objets. Très utilisée en marketing (segmentation de clientèle), cette stratégie n'a pas grand intérêt sur ces données. Elle est développée dans les calepins du [dépot d'exploration](https://github.com/wikistat/Exploration), par exemple celui des données bancaires (carte visa)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "10",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
